{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"M2HnzwZ5VByd"},"outputs":[],"source":["# %cd drive/MyDrive/dl-assigment-2/\n","\n","# !unzip nature_12K.zip"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-03-30T15:55:53.946152Z","iopub.status.busy":"2024-03-30T15:55:53.945319Z","iopub.status.idle":"2024-03-30T15:56:02.767356Z","shell.execute_reply":"2024-03-30T15:56:02.766567Z","shell.execute_reply.started":"2024-03-30T15:55:53.946115Z"},"id":"Zf1EZbdVTaIi","trusted":true},"outputs":[],"source":["import cv2\n","import glob\n","import random\n","import numpy as np\n","import torch\n","from pandas.core.common import flatten\n","torch.manual_seed(7)\n","torch.cuda.empty_cache()\n","from torchvision import transforms\n","from torch.utils.data import DataLoader,Dataset\n","from PIL import Image\n","from torch import  nn,optim\n","import torch.nn.functional as F\n","from tqdm import tqdm "]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-30T15:56:10.828840Z","iopub.status.busy":"2024-03-30T15:56:10.828319Z","iopub.status.idle":"2024-03-30T15:56:11.015198Z","shell.execute_reply":"2024-03-30T15:56:11.014224Z","shell.execute_reply.started":"2024-03-30T15:56:10.828809Z"},"id":"grem9A74d4Gd","trusted":true},"outputs":[],"source":["# constants\n","IMG_MODE = 'RGB'\n","TRAIN_LABEL = 'train'\n","TEST_LABEL = 'test'\n","\n","class iNaturalist(Dataset):\n","    def __init__(self, image_paths, class_to_idx, transform):\n","        self.all_images = image_paths\n","        self.current_transform = transform\n","        self.class_to_idx = class_to_idx\n","        \n","    def __len__(self):\n","        return len(self.all_images)\n","\n","    def __getitem__(self, idx):\n","        image_filepath = self.all_images[idx]\n","        image = cv2.imread(image_filepath)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        \n","        y = self.class_to_idx[image_filepath.split('/')[-2]]\n","        \n","        X = Image.fromarray(np.uint8(image)).convert(IMG_MODE)\n","        X = Image.fromarray(image.astype('uint8'), IMG_MODE)\n","        X = self.current_transform(X)\n","\n","        return X, y\n","\n","def create_data(data_type, data_path,  data_aug, image_shape, b_size):\n","    classes = [image_path.split('/')[-1] for image_path in glob.glob(data_path + '/*')]\n","\n","    all_images = [glob.glob(image_path + '/*') for image_path in glob.glob(data_path + '/*')]\n","    all_images = list(flatten(all_images))\n","\n","    idx_to_class,class_to_idx = dict(),dict()\n","    for i, j in enumerate(classes):\n","        idx_to_class[i] = j\n","        class_to_idx[j] = i\n","\n","    non_aug_tran = transforms.Compose([transforms.Resize((image_shape)),\n","                                transforms.ToTensor()\n","                                    ])\n","    if data_type == TEST_LABEL:\n","        test_image_paths=all_images\n","        test_dataset= iNaturalist(test_image_paths,class_to_idx,non_aug_tran)\n","        test_loader = DataLoader(test_dataset, batch_size=b_size, shuffle=True)\n","\n","        return test_loader\n","\n","\n","    random.shuffle(all_images)\n","\n","    tr_paths, v_paths = all_images[:int(0.9*len(all_images))], all_images[int(0.9*len(all_images)):] \n","\n","    tr_data,v_data = iNaturalist(tr_paths,class_to_idx,non_aug_tran),iNaturalist(v_paths,class_to_idx,non_aug_tran)\n","\n","    if data_aug:\n","        augu_tran = transforms.Compose([transforms.Resize((image_shape)),\n","                transforms.RandomRotation(degrees=30),\n","                transforms.RandomHorizontalFlip(p=0.5),\n","                transforms.RandomGrayscale(p=0.2),\n","                transforms.ToTensor(),\n","                            ])\n","\n","        tr_data = iNaturalist(tr_paths,class_to_idx,augu_tran)\n","        v_data = iNaturalist(v_paths,class_to_idx,augu_tran)  \n","\n","    t_loader,v_loader = DataLoader(tr_data, batch_size=b_size, shuffle=True),DataLoader(v_data, batch_size=b_size, shuffle=True)\n","    return t_loader,v_loader\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class ConvolutionBlocks(nn.Module):\n","    def __init__(self, activation, batch_norm, size_filters, filter_organization, number_filters,num_conv_layers):\n","        super().__init__()\n","        self.activationFn=activation\n","        self.num_filters=[number_filters]\n","        self.batch_norm=batch_norm\n","        for i in range(1,num_conv_layers):\n","          self.num_filters.append(int(self.num_filters[i-1]*filter_organization))\n","\n","        self.conv1 = nn.Conv2d(in_channels=3,out_channels=self.num_filters[0],kernel_size=size_filters[0],stride=(1, 1),padding=(1, 1),bias=False)\n","        self.conv2 = nn.Conv2d(in_channels=self.num_filters[0],out_channels=self.num_filters[1],kernel_size=size_filters[1],stride=(1, 1),padding=(1, 1),bias=False)\n","        self.conv3 = nn.Conv2d(in_channels=self.num_filters[1],out_channels=self.num_filters[2],kernel_size=size_filters[2],stride=(1, 1),padding=(1, 1),bias=False)\n","        self.conv4 = nn.Conv2d(in_channels=self.num_filters[2],out_channels=self.num_filters[3],kernel_size=size_filters[3],stride=(1, 1),padding=(1, 1),bias=False)\n","        self.conv5 = nn.Conv2d(in_channels=self.num_filters[3],out_channels=self.num_filters[4],kernel_size=size_filters[4],stride=(1, 1),padding=(1, 1),bias=False)\n","        self.pool  = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n","        self.batchnorm1 = nn.BatchNorm2d(self.num_filters[0])\n","        self.batchnorm2 = nn.BatchNorm2d(self.num_filters[1])\n","        self.batchnorm3 = nn.BatchNorm2d(self.num_filters[2])\n","        self.batchnorm4 = nn.BatchNorm2d(self.num_filters[3])\n","        self.batchnorm5 = nn.BatchNorm2d(self.num_filters[4])\n","\n","    def forward(self, x):\n","        if self.batch_norm == False: \n","            x=self.pool(self.activationFn(self.conv1(x)))\n","            x=self.pool(self.activationFn(self.conv2(x)))\n","            x=self.pool(self.activationFn(self.conv3(x)))\n","            x=self.pool(self.activationFn(self.conv4(x)))\n","            x=self.pool(self.activationFn(self.conv5(x)))\n","            return x\n","        else:\n","            x= self.pool(self.activationFn(self.batchnorm1(self.conv1(x))))\n","            x= self.pool(self.activationFn(self.batchnorm2(self.conv2(x))))\n","            x= self.pool(self.activationFn(self.batchnorm3(self.conv3(x))))\n","            x= self.pool(self.activationFn(self.batchnorm4(self.conv4(x))))\n","            x= self.pool(self.activationFn(self.batchnorm5(self.conv5(x))))\n","            return x\n","\n","class Model(nn.Module):\n","    def __init__(self, image_shape,dropout , activation, batch_norm, size_filters, filter_organization, \n","                  number_initial_filters , neurons_in_dense_layer,num_conv_layers):\n","        super().__init__()\n","        if activation=='relu':\n","            self.activation = nn.ReLU()\n","        else:\n","            self.activation =nn.LeakyReLU()\n","        self.conv_blocks = ConvolutionBlocks(activation = self.activation,\n","                                             batch_norm= batch_norm,\n","                                             size_filters= size_filters,\n","                                             filter_organization= filter_organization,\n","                                             number_filters= number_initial_filters,\n","                                             num_conv_layers= num_conv_layers)\n","\n","        sz=self.conv_blocks(torch.zeros(*(image_shape))).data.shape\n","        self.fully_conn_layer_1   = nn.Linear(sz[1] * sz[2] * sz[3],neurons_in_dense_layer,bias=True)  \n","        self.output_layer= nn.Linear(neurons_in_dense_layer,10,bias=True)   \n","        self.dropout=nn.Dropout(p=dropout)\n","        self.num_conv_layers = num_conv_layers\n","    def forward(self, x):\n","        x = self.conv_blocks(x)\n","        x = self.dropout(self.activation(self.fully_conn_layer_1(x.reshape(x.shape[0],-1))))\n","        x = F.softmax(self.output_layer(x),dim=1)\n","        return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def convertIntoPercentage(x,n,digit=4):\n","    return round((x / n) * 100, digit)\n","\n","def evaluate(device, loader, model):\n","    ''' Function to calculate accuracy to see performance of our model '''\n","        \n","    Y_cap_num,N_val = 0,0\n","    loss = 0\n","\n","    model.eval()\n","\n","    with torch.no_grad():\n","        for X, Y in tqdm(loader, total=len(loader)):\n","            X,Y = X.to(device=device),Y.to(device=device)\n","\n","            Y_cap = model(X)\n","            loss += nn.CrossEntropyLoss()(Y_cap, Y).item()\n","\n","            _, predictions = Y_cap.max(1)\n","\n","            N_val = N_val + predictions.size(0)\n","            \n","            Y_cap_num = Y_cap_num +  (predictions == Y).sum().item()\n","           \n","    acc = convertIntoPercentage(Y_cap_num , N_val)\n","    loss = loss/N_val\n","    return acc, loss\n","\n","def train(args):\n","\n","    torch.cuda.empty_cache()\n","    image_shape = (1,3,224,224)\n","    test_data_path = '/kaggle/input/nature-12k/inaturalist_12K/val/'\n","    train_data_path = '/kaggle/input/nature-12k/inaturalist_12K/train/'\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    model = Model(image_shape= image_shape,\n","                  dropout= args.dropout,\n","                  activation= args.activation,\n","                  batch_norm= args.batch_norm,\n","                  size_filters= args.size_filters,\n","                  filter_organization= args.filter_organization,\n","                  number_initial_filters= args.number_initial_filters,\n","                  neurons_in_dense_layer= args.neurons_in_dense_layer,\n","                  num_conv_layers= 5\n","                ).to(device)\n","\n","    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n","\n","    for epoch in range(args.epochs):\n","        model.train()\n","        test_loader = create_data(\"test\",test_data_path,args.data_aug, image_shape[2:], args.batch_size)\n","        train_loader, valid_loader = create_data(\"train\",train_data_path,args.data_aug,image_shape[2:], args.batch_size)\n","\n","        train_correct, train_loss = 0, 0\n","        total_samples = 0\n","        for batch_id,(data,label) in enumerate(tqdm(train_loader)):\n","          \n","            data = data.to(device=device)\n","            targets = label.to(device=device)\n","\n","            scores = model(data)\n","            loss = nn.CrossEntropyLoss()(scores, targets)\n","            train_loss += loss.item()\n","            \n","            _, predictions = scores.max(1)\n","            train_correct += (predictions == targets).sum()\n","            total_samples +=  predictions.size(0)\n","            \n","            optimizer.zero_grad()\n","            loss.backward()\n","\n","            optimizer.step()\n","\n","        \n","        train_loss /= total_samples\n","        train_acc = round((train_correct / total_samples).item()  * 100, 4)\n","        \n","       \n","        \n","        val_acc, val_loss = evaluate(device, valid_loader, model)\n","        test_acc, test_loss = evaluate(device, test_loader, model)\n","        \n","        \n","        print('\\nEpoch ', epoch, 'train_acc', train_acc, 'val_acc', val_acc, 'test_acc', test_acc, 'train_loss', train_loss, 'val_loss', val_loss, 'test_loss', test_loss) "]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-30T15:56:12.987919Z","iopub.status.busy":"2024-03-30T15:56:12.987550Z","iopub.status.idle":"2024-03-30T15:58:22.074828Z","shell.execute_reply":"2024-03-30T15:58:22.073399Z","shell.execute_reply.started":"2024-03-30T15:56:12.987893Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":[" 46%|████▌     | 65/141 [02:04<02:25,  1.91s/it]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(dictionary)\n\u001b[1;32m     18\u001b[0m args \u001b[38;5;241m=\u001b[39m DotDict(config)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[2], line 198\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    196\u001b[0m train_correct, train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    197\u001b[0m total_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 198\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_id,(data,label) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(train_loader)):\n\u001b[1;32m    199\u001b[0m   \n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# Get data to cuda if possible\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    202\u001b[0m     targets \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[0;32mIn[2], line 13\u001b[0m, in \u001b[0;36miNaturalist.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     12\u001b[0m     image_filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_images[idx]\n\u001b[0;32m---> 13\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_filepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m     16\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_to_idx[image_filepath\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]]\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["config = {\n","    \"epochs\" : 10,\n","    \"batch_size\": 64,\n","    'activation': 'relu',\n","    'learning_rate':0.001,\n","    \"dropout\": 0.3,\n","    \"batch_norm\": True,\n","    \"data_aug\": True,\n","    'size_filters':[7,5,5,3,3],\n","    'filter_organization': 2,\n","    'number_initial_filters': 16,\n","    \"neurons_in_dense_layer\": 512\n","}\n","class DotDict:\n","    def __init__(self, dictionary):\n","        self.__dict__.update(dictionary)\n","\n","args = DotDict(config)\n","train(args)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4693720,"sourceId":7975703,"sourceType":"datasetVersion"}],"dockerImageVersionId":30673,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
